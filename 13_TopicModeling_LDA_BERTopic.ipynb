{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOODR1IttPa1FtgY/p2HVEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/NLP/blob/main/13_TopicModeling_LDA_BERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'red'> üêπ üëÄ üêæ **Text/Content/Web Scraping without HTML tags**\n",
        "\n",
        "## **API-based Data Collection**\n",
        "\n",
        "### <font color = 'blue'> **cf., Crawling (a.k.a. HTML Scraping) or Text Mining**"
      ],
      "metadata": {
        "id": "s6Mus7Nq34xo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUKngVPW2PTG"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "\n",
        "import requests #Import the requests library to make HTTP requests.\n",
        "\n",
        "def get_wikipedia_page(title):                   #Define a function\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"  #Set the API(application program interface) endpoint URL: https://en.wikipedia.org/w/api.php.\n",
        "\n",
        "    PARAMS = {                                  #Build PARAMS (query parameters) for the API request:\n",
        "        \"action\": \"query\",                      #ask the API to run a query\n",
        "        \"format\": \"json\",                       #request a JSON response\n",
        "        \"prop\": \"extracts\",                     #ask for the page extract (clean text summary)\n",
        "        \"titles\": title,                        #specify which page to fetch (by title)\n",
        "        \"explaintext\": 1                        #return plain text (no HTML/markup)\n",
        "\n",
        "    }\n",
        "\n",
        "    # IMPORTANT: Error messages for Bots pretending to be browsers. Do not pretend you are browsers.\n",
        "    #headers = {\n",
        "    #    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \" #header to mimic a normal browser request (helps avoid blocks)\n",
        "    #                  \"(KHTML, like Gecko) Chrome/123.0 Safari/537.36\"\n",
        "    #}\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"MyNLPProject (education use)\"\n",
        "        }\n",
        "    response = requests.get(URL, params=PARAMS, headers=headers)           #Send a GET request to the API with requests\n",
        "\n",
        "    if response.status_code != 200:                                        #Check the HTTP status code: If not 200 OK, print an error message and return None.\n",
        "        print(\"HTTP error:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        data = response.json()                                            #Try to parse the response body as JSON with response.json():\n",
        "    except:\n",
        "        print(\"JSON decode error\")                                        #If JSON decoding fails, print a debug message showing the start of the raw response and return None.\n",
        "        print(\"Raw response:\", response.text[:500])\n",
        "        return None\n",
        "\n",
        "    pages = data.get(\"query\", {}).get(\"pages\", {})                      #Navigate the JSON structure to the page data: data[\"query\"][\"pages\"] (a dictionary keyed by numeric page id).\n",
        "    page = next(iter(pages.values()))                                   #Extract the single page object with next(iter(pages.values())) (handles the unknown page id).\n",
        "    return page.get(\"extract\", \"\")                                      #Return the page‚Äôs plain-text extract via page.get(\"extract\", \"\").\n",
        "                                                                        #If the page exists, this is the article text; if not, it returns an empty string (or None earlier if errors occurred).\n",
        "\n",
        "# üêπüêæ **Install NLTK and Download necessary models**\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# üêπüêæ **1Ô∏è‚É£ Pandas Library**\n",
        "!pip install pandas\n",
        "!pip install lexical_diversity\n",
        "import pandas as pd #Import Pandas Package\n",
        "import lexical_diversity as ld\n",
        "\n",
        "\n",
        "# üÖ∞Ô∏è **Group1**\n",
        "# ‚úÖ **Text scraping for Group1**\n",
        "titles = [\n",
        "    \"K-pop\",\n",
        "    \"Korean Wave\",\n",
        "    \"KPop Demon Hunters\",\n",
        "    \"BTS\"\n",
        "]\n",
        "\n",
        "corpus = {}\n",
        "\n",
        "for t in titles:\n",
        "    txt = get_wikipedia_page(t)\n",
        "    if txt:\n",
        "        corpus[t] = txt\n",
        "    else:\n",
        "        print(\"Failed:\", t)\n",
        "\n",
        "# Show first 200 chars for each\n",
        "for title, text in corpus.items():\n",
        "    print(\"\\n====\", title, \"====\")\n",
        "    print(text[:200])\n",
        "\n",
        "# üêπ üêæ üìå **Use this!!!**üìå\n",
        "# ‚≠ï <font color = 'green'> **Script for [Group1] ‚Äî Create one Txt file with records separated by @@@@@**\n",
        "\n",
        "output = []\n",
        "\n",
        "for title in titles:\n",
        "    txt = get_wikipedia_page(title)\n",
        "    if not txt:\n",
        "        txt = \"\"   # store empty if missing\n",
        "    block = f\"@@@@@\\nTITLE: {title}\\n{txt}\\n\"\n",
        "    output.append(block)\n",
        "\n",
        "final = \"\\n\".join(output)\n",
        "\n",
        "with open(\"wiki_corpus_delimited_group1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final)\n",
        "\n",
        "print(\"Saved: wiki_corpus_delimited_group1.txt\")\n",
        "\n",
        "\n",
        "# üêπüêæ **Read the txt file**\n",
        "# üê£ **Open and read the text file for Group1**\n",
        "\n",
        "# ‚ñ∂Ô∏è Step 1: You need to modify this codeline üçéüçéüçéüçéüçé\n",
        "file = open(\"/content/wiki_corpus_delimited_group1.txt\", 'rt')\n",
        "\n",
        "txt = file.read()\n",
        "print(txt)\n",
        "file.close() #Using this close()function, you are no longer using your text file of the current workingdirectory with open()function.\n",
        "\n",
        "\n",
        "\n",
        "##üêπüêæ ‚ùÑÔ∏è **Basic Cleaning**\n",
        "###**üìçApply a series of functions for replacement in Group1**\n",
        "# STEP 2: Clean the text\n",
        "\n",
        "import re\n",
        "\n",
        "# Step 1: Read file to change path as needed üçéüçéüçéüçéüçéüçé\n",
        "with open(\"/content/wiki_corpus_delimited_group1.txt\", 'rt') as fl:\n",
        "    raw_text = fl.read()\n",
        "\n",
        "clean_text = (\n",
        "    raw_text\n",
        "    .replace(\"\\n\", \" \")\n",
        "    .replace(\"‚Äú\", \"\")\n",
        "    .replace(\"‚Äù\", \"\")\n",
        "    .replace(\"\\\"\", \"\")\n",
        "    .replace(\"/\", \"\")\n",
        "    .replace(\"_\", \"\")\n",
        "    .replace(\"===\", \"\")\n",
        "    .replace(\"==\", \"\")\n",
        "    .replace(\"=\", \"\")\n",
        "    .replace(\"*\", \"\")\n",
        "    .replace(\"?\", \"\")\n",
        "    .replace(\"!\", \"\")\n",
        "    .replace(\"--\", \" \")\n",
        "    .replace(\"(\", \"\")\n",
        "    .replace(\")\", \"\")\n",
        ")\n",
        "\n",
        "# STEP 3: Save the cleaned content to a NEW file as you designate the output path üçèüçèüçèüçèüçèüçè\n",
        "output_path = \"/content/wiki_corpus_delimited_group1_CLEANED.txt\"\n",
        "with open(output_path, 'w') as cf:\n",
        "    cf.write(clean_text) #Get content named 'clean_text' to the new empty file\n",
        "\n",
        "# Optional: Print to verify\n",
        "print(\"‚úÖ Cleaned text saved to:\", output_path)\n",
        "\n",
        "\n",
        "# ‚úÖ ‚úÖ**Text scraping for Group2**\n",
        "titles = [\n",
        "    \"2024 Nobel Prize in Literature\",\n",
        "    \"Han Kang\",\n",
        "    \"Bong Joon Ho\",\n",
        "    \"Pachinko\"\n",
        "]\n",
        "\n",
        "corpus = {}\n",
        "\n",
        "for t in titles:\n",
        "    txt = get_wikipedia_page(t)\n",
        "    if txt:\n",
        "        corpus[t] = txt\n",
        "    else:\n",
        "        print(\"Failed:\", t)\n",
        "\n",
        "# Show first 200 chars for each\n",
        "for title, text in corpus.items():\n",
        "    print(\"\\n====\", title, \"====\")\n",
        "    print(text[:200])\n",
        "\n",
        "# ‚≠ï‚≠ï <font color = 'blue'> **Script for [Group2] ‚Äî Create one Txt file with records separated by @@@@@**\n",
        "output = []\n",
        "\n",
        "for title in titles:\n",
        "    txt = get_wikipedia_page(title)\n",
        "    if not txt:\n",
        "        txt = \"\"   # store empty if missing\n",
        "    block = f\"@@@@@\\nTITLE: {title}\\n{txt}\\n\"\n",
        "    output.append(block)\n",
        "\n",
        "final = \"\\n\".join(output)\n",
        "\n",
        "with open(\"wiki_corpus_delimited_group2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final)\n",
        "\n",
        "print(\"Saved: wiki_corpus_delimited_group2.txt\")\n",
        "\n",
        "\n",
        "# üê£üê£ **Open and read the text file for Group2**\n",
        "# ‚ñ∂Ô∏è Step 1: You need to modify this codeline üçéüçéüçéüçéüçé\n",
        "file = open(\"/content/wiki_corpus_delimited_group2.txt\", 'rt')\n",
        "\n",
        "txt = file.read()\n",
        "print(txt)\n",
        "file.close() #Using this close()function, you are no longer using your text file of the current workingdirectory with open()function.\n",
        "\n",
        "\n",
        "\n",
        "# **üìçüìçApply a series of functions for replacement in Group2**\n",
        "import re\n",
        "\n",
        "# Step 1: Read file to change path as needed üçéüçéüçéüçéüçéüçé\n",
        "with open(\"/content/wiki_corpus_delimited_group2.txt\", 'rt') as fl:\n",
        "    raw_text = fl.read()\n",
        "\n",
        "# STEP 2: Clean the text\n",
        "clean_text = (\n",
        "    raw_text\n",
        "    .replace(\"\\n\", \" \")\n",
        "    .replace(\"‚Äú\", \"\")\n",
        "    .replace(\"‚Äù\", \"\")\n",
        "    .replace(\"\\\"\", \"\")\n",
        "    .replace(\"/\", \"\")\n",
        "    .replace(\"_\", \"\")\n",
        "    .replace(\"===\", \"\")\n",
        "    .replace(\"==\", \"\")\n",
        "    .replace(\"=\", \"\")\n",
        "    .replace(\"*\", \"\")\n",
        "    .replace(\"?\", \"\")\n",
        "    .replace(\"!\", \"\")\n",
        "    .replace(\"--\", \" \")\n",
        "    .replace(\"(\", \"\")\n",
        "    .replace(\")\", \"\")\n",
        ")\n",
        "\n",
        "# STEP 3: Save the cleaned content to a NEW file as you designate the output path üçèüçèüçèüçèüçèüçè\n",
        "output_path = \"/content/wiki_corpus_delimited_group2_CLEANED.txt\"\n",
        "with open(output_path, 'w') as cf:\n",
        "    cf.write(clean_text) #Get content named 'clean_text' to the new empty file\n",
        "\n",
        "# Optional: Print to verify\n",
        "print(\"‚úÖ Cleaned text saved to:\", output_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#üêπüê£**Clone your github repository of your interest**\n",
        "!git clone https://github.com/ms624atyale/NLP\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xJfuDBKZV_3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'red'> **üîµ PART 1 ‚Äî LDA**"
      ],
      "metadata": {
        "id": "BLNoIBuuWAnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Install Required Packages\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "\n",
        "\n",
        "#üìå LDA Code (Works for Group1 or Group2)\n",
        "# üíä Change file path depending on which group you want.\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# üîµ Choose file (change path if needed)\n",
        "file_path = \"/content/wiki_corpus_delimited_group1_CLEANED.txt\"\n",
        "# file_path = \"/content/wiki_corpus_delimited_group2_CLEANED.txt\"\n",
        "\n",
        "# STEP 1Ô∏è‚É£ Read cleaned file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# STEP 2Ô∏è‚É£ Split documents using delimiter\n",
        "documents = text.split(\"@@@@@\")\n",
        "documents = [doc.strip() for doc in documents if len(doc.strip()) > 0]\n",
        "\n",
        "# STEP 3Ô∏è‚É£ Preprocess\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "processed_docs = []\n",
        "for doc in documents:\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    processed_docs.append(tokens)\n",
        "\n",
        "# STEP 4Ô∏è‚É£ Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# STEP 5Ô∏è‚É£ Train LDA\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=3,\n",
        "    passes=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# STEP 6Ô∏è‚É£ Print topics\n",
        "print(\"\\nüü¢ LDA Topics:\\n\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "\n",
        "#üîé Optional: Show Topic Distribution Per Document\n",
        "for i, row in enumerate(lda_model[corpus]):\n",
        "    print(f\"\\nDocument {i} topic distribution:\")\n",
        "    print(row)\n"
      ],
      "metadata": {
        "id": "xqU__SVU_BjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'red'> **üîµ PART 2 ‚Äî BERTopic Model**"
      ],
      "metadata": {
        "id": "ikVDgH5RPLP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîµ PART 2 ‚Äî BERTopic Model using Sentence Transformers, UMAP, HDBSCAN, c-TF-IDF\n",
        "\n",
        "#üìå Install BERTopic\n",
        "!pip install bertopic\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# üìå BERTopic Code\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# üîµ Choose file\n",
        "file_path = \"/content/wiki_corpus_delimited_group1_CLEANED.txt\"\n",
        "# file_path = \"/content/wiki_corpus_delimited_group2_CLEANED.txt\"\n",
        "\n",
        "# STEP 1Ô∏è‚É£ Read cleaned file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# STEP 2Ô∏è‚É£ Split documents\n",
        "documents = text.split(\"@@@@@\")\n",
        "documents = [doc.strip() for doc in documents if len(doc.strip()) > 0]\n",
        "\n",
        "# STEP 3Ô∏è‚É£ Initialize BERTopic\n",
        "topic_model = BERTopic(\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# STEP 4Ô∏è‚É£ Fit model\n",
        "topics, probs = topic_model.fit_transform(documents)\n",
        "\n",
        "# STEP 5Ô∏è‚É£ Show topic info\n",
        "print(\"\\nüîµ BERTopic Summary:\\n\")\n",
        "print(topic_model.get_topic_info())\n",
        "\n",
        "# STEP 6Ô∏è‚É£ Print words per topic\n",
        "for topic_num in set(topics):\n",
        "    print(f\"\\nTopic {topic_num}:\")\n",
        "    print(topic_model.get_topic(topic_num))\n",
        "\n",
        "\n",
        "\n",
        "# üîé Optional Visualization\n",
        "topic_model.visualize_topics()\n",
        "\n"
      ],
      "metadata": {
        "id": "EjW0FWQHOawf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'blue'> 1Ô∏è‚É£ **Combine Group1 + Group2**"
      ],
      "metadata": {
        "id": "b3J0ZylcPZd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine cleaned files\n",
        "file_path1 = \"/content/wiki_corpus_delimited_group1_CLEANED.txt\"\n",
        "file_path2 = \"/content/wiki_corpus_delimited_group2_CLEANED.txt\"\n",
        "\n",
        "with open(file_path1, \"r\", encoding=\"utf-8\") as f1:\n",
        "    text1 = f1.read()\n",
        "\n",
        "with open(file_path2, \"r\", encoding=\"utf-8\") as f2:\n",
        "    text2 = f2.read()\n",
        "\n",
        "combined_text = text1 + \"\\n\" + text2\n",
        "\n",
        "documents = combined_text.split(\"@@@@@\")\n",
        "documents = [doc.strip() for doc in documents if len(doc.strip()) > 0]\n",
        "\n",
        "print(\"Total documents:\", len(documents))"
      ],
      "metadata": {
        "id": "vMuqfOEXPkC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ **PART 1 ‚Äî LDA with Coherence Score using Gensim LDA & CoherenceModel (c_v score)**"
      ],
      "metadata": {
        "id": "xuKBAU6dPulF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Preprocess\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "processed_docs = []\n",
        "for doc in documents:\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    tokens = [w for w in tokens if w.isalpha()]\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    processed_docs.append(tokens)\n",
        "\n",
        "# Dictionary & Corpus\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Train LDA\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=4,\n",
        "    passes=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print topics\n",
        "print(\"\\nüü¢ LDA Topics:\\n\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "\n",
        "# Compute coherence\n",
        "coherence_model = CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=processed_docs,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "\n",
        "lda_coherence = coherence_model.get_coherence()\n",
        "print(\"\\nüü¢ LDA Coherence Score:\", lda_coherence)\n"
      ],
      "metadata": {
        "id": "0pweoX8kP2m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† How to Interpret Coherence\n",
        "\n",
        "0.3 - weak\n",
        "\n",
        "0.4 - decent\n",
        "\n",
        "0.5+ - good\n",
        "\n",
        "0.6+ - strong (for small datasets)\n",
        "\n",
        "Higher = more semantically consistent topics."
      ],
      "metadata": {
        "id": "nswj8n8BP-Ol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ PART 2 ‚Äî BERTopic + Coherence\n",
        "\n",
        "BERTopic doesn‚Äôt directly compute coherence,\n",
        "so we extract topic words and compute coherence manually."
      ],
      "metadata": {
        "id": "41ItGTiHQRG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "\n",
        "# Adjust UMAP for small dataset\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=3,      # must be < number of documents\n",
        "    n_components=2,\n",
        "    min_dist=0.0,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Adjust HDBSCAN for small dataset\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=2,\n",
        "    min_samples=1,\n",
        "    metric='euclidean',\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "# Initialize BERTopic with custom models\n",
        "topic_model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(documents)\n",
        "\n",
        "print(topic_model.get_topic_info())\n"
      ],
      "metadata": {
        "id": "WNyWmzfXQSrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä STEP 3 ‚Äî Compare LDA vs BERTopic Quality"
      ],
      "metadata": {
        "id": "yU1vMDSCR3W-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üü¢ PART 1 ‚Äî LDA QUALITY METRICS"
      ],
      "metadata": {
        "id": "zBl0q0hZSrdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "import numpy as np\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Preprocess\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "processed_docs = []\n",
        "for doc in documents:\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    tokens = [w for w in tokens if w.isalpha()]\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    processed_docs.append(tokens)\n",
        "\n",
        "# Dictionary & Corpus\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Train LDA\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=4,\n",
        "    passes=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -------- Coherence --------\n",
        "coherence_model_lda = CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=processed_docs,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "\n",
        "lda_coherence = coherence_model_lda.get_coherence()\n",
        "\n",
        "# -------- Topic Diversity --------\n",
        "def topic_diversity(topics, top_k=10):\n",
        "    unique_words = set()\n",
        "    total_words = 0\n",
        "    for topic in topics:\n",
        "        words = topic[:top_k]\n",
        "        unique_words.update(words)\n",
        "        total_words += top_k\n",
        "    return len(unique_words) / total_words\n",
        "\n",
        "lda_topics = []\n",
        "for i in range(4):\n",
        "    words = [word for word, prob in lda_model.show_topic(i, topn=10)]\n",
        "    lda_topics.append(words)\n",
        "\n",
        "lda_diversity = topic_diversity(lda_topics)\n",
        "\n",
        "print(\"\\nüü¢ LDA Results\")\n",
        "print(\"Coherence:\", round(lda_coherence, 4))\n",
        "print(\"Topic Diversity:\", round(lda_diversity, 4))\n",
        "print(\"Number of Topics:\", 4)\n"
      ],
      "metadata": {
        "id": "poc-9HTfR6X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ PART 2 ‚Äî BERTopic QUALITY METRICS"
      ],
      "metadata": {
        "id": "99uICVC5SuO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "\n",
        "# Adjust for small dataset\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=3,\n",
        "    n_components=2,\n",
        "    min_dist=0.0,\n",
        "    metric='cosine',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=2,\n",
        "    min_samples=1,\n",
        "    metric='euclidean',\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(documents)\n",
        "\n",
        "# -------- Extract topic words --------\n",
        "topic_info = topic_model.get_topics()\n",
        "\n",
        "bertopic_topics = []\n",
        "for topic_id in topic_info:\n",
        "    if topic_id == -1:\n",
        "        continue\n",
        "    words = [word for word, _ in topic_info[topic_id][:10]]\n",
        "    bertopic_topics.append(words)\n",
        "\n",
        "# -------- Coherence --------\n",
        "coherence_model_bertopic = CoherenceModel(\n",
        "    topics=bertopic_topics,\n",
        "    texts=processed_docs,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "\n",
        "bertopic_coherence = coherence_model_bertopic.get_coherence()\n",
        "\n",
        "# -------- Topic Diversity --------\n",
        "bertopic_diversity = topic_diversity(bertopic_topics)\n",
        "\n",
        "# -------- Outlier Ratio --------\n",
        "outlier_ratio = topics.count(-1) / len(topics)\n",
        "\n",
        "print(\"\\nüîµ BERTopic Results\")\n",
        "print(\"Coherence:\", round(bertopic_coherence, 4))\n",
        "print(\"Topic Diversity:\", round(bertopic_diversity, 4))\n",
        "print(\"Number of Topics:\", len(bertopic_topics))\n",
        "print(\"Outlier Ratio:\", round(outlier_ratio, 4))\n"
      ],
      "metadata": {
        "id": "5Ku2_NylS4Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä FINAL SIDE-BY-SIDE COMPARISON"
      ],
      "metadata": {
        "id": "c33gCX_uTCvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìä MODEL COMPARISON\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"LDA Coherence:        {round(lda_coherence,4)}\")\n",
        "print(f\"BERTopic Coherence:   {round(bertopic_coherence,4)}\")\n",
        "print()\n",
        "print(f\"LDA Diversity:        {round(lda_diversity,4)}\")\n",
        "print(f\"BERTopic Diversity:   {round(bertopic_diversity,4)}\")\n",
        "print()\n",
        "print(f\"LDA Topics:           4\")\n",
        "print(f\"BERTopic Topics:      {len(bertopic_topics)}\")\n",
        "print(f\"BERTopic Outliers:    {round(outlier_ratio,4)}\")\n"
      ],
      "metadata": {
        "id": "cZb_rPl9TEXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† How to Interpret Results\n",
        "üîπ Coherence\n",
        "\n",
        "Higher = more semantically meaningful topics.\n",
        "\n",
        "üîπ Topic Diversity\n",
        "\n",
        "Closer to 1 = topics share fewer repeated words.\n",
        "\n",
        "üîπ Outlier Ratio (BERTopic only)\n",
        "\n",
        "Higher = unstable clustering (common in small datasets).\n",
        "\n",
        "# üéØ üç∞ üç® üç¨  <font color = 'green'> **My Note:**\n",
        "# üêπüêæ üå± <font color = 'green'> **Visit my ChatGPT for <Topic Modeling Clarification> for interpretations!**"
      ],
      "metadata": {
        "id": "Q-8wGRsfTUD4"
      }
    }
  ]
}